{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large language model has a quantitatively large parameters that defines how a model will take an inout and give us the output\n",
    "\n",
    "The properties in LLM which do not appear in smaller models is zero shot learning - it is the capability of the model to do a task which it was not explicitly trained to do before\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recent LLM models use self supervised learning wherein it does not require manual labelling of each example in each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs work on \"next word prediction paradigm\" which defines the next word based on the highest probability of the next word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works on autoregression : what is the probability of the Nth token given the preceedings of the m tokens before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 properties of using LLMs : \n",
    "1. Prompt engineering\n",
    "2. Model fine-tuning\n",
    "3. Build your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 1 : Prompt Engineering is using the LLM out of the box i.e. not touching any of the model parameters\n",
    "\n",
    "The easy way is to use the interface directly whereas the less easy way is the OpenAI API or hugging face transformers library. \n",
    "\n",
    "Hugging face gives access to models which you can run it on your system locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 : Model Finetuning is where you are adjusting atleast one model parameters(not all of them)\n",
    "\n",
    "Task 1: Obtained pre-trained LLM\n",
    "Task 2: Update model parameters giving task specific examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 3:  Build your LLM\n",
    "- Get your large data from different sources and preprocess it into a training dataset\n",
    "- Take the data and you can do the model training and you can take the pre-trained LLM and go from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OpenAI (Python) API | Introduction & Example Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An API is  a way to interact with your remote application programatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an API helps us get acess to unique features which are not available through the web interface.\n",
    "\n",
    "1. Customizable system message.\n",
    "2. Adjust Input Parameters - eg: max response length, number of responses and temperature \n",
    "3. process images and other file types\n",
    "4. Extract helpful embeddings for downstream tasks\n",
    "5. Input audio for transcription and translation\n",
    "6. Model fine-tuning functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are a set of words and characters represented by a set of numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"***************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create chat completion\n",
    "chat_completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", messages = [{\"role\":\"user\",\"content\":\"Listen to your\"}])\n",
    "\n",
    "#Messages are always a list of dictionaries\n",
    "#Instead of user - you can also use assistant - to define some text generated by chatbot, system - for system message etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-8oaidGrTAPXpRSEmKfkjAwXOqittP',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1707068239,\n",
       " 'model': 'gpt-3.5-turbo-0613',\n",
       " 'choices': [<OpenAIObject at 0x1db924b2ff0> JSON: {\n",
       "    \"index\": 0,\n",
       "    \"message\": {\n",
       "      \"role\": \"assistant\",\n",
       "      \"content\": \"heart, it knows what you truly desire. Trust in your instincts and follow the path that feels right for you. Ignore the noise and opinions of others, and instead, focus on what brings you joy and fulfillment. Your heart will guide you towards your true purpose and happiness. Embrace the journey and trust in yourself.\"\n",
       "    },\n",
       "    \"logprobs\": null,\n",
       "    \"finish_reason\": \"stop\"\n",
       "  }],\n",
       " 'usage': <OpenAIObject at 0x1db924b31d0> JSON: {\n",
       "   \"prompt_tokens\": 10,\n",
       "   \"completion_tokens\": 64,\n",
       "   \"total_tokens\": 74\n",
       " },\n",
       " 'system_fingerprint': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choices field in the above JSON is what holds the response from the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart, it knows what you truly desire. Trust in your instincts and follow the path that feels right for you. Ignore the noise and opinions of others, and instead, focus on what brings you joy and fulfillment. Your heart will guide you towards your true purpose and happiness. Embrace the journey and trust in yourself.\n"
     ]
    }
   ],
   "source": [
    "#print the chat completion\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart\n"
     ]
    }
   ],
   "source": [
    "#Setting max tokens\n",
    "#Create chat completion\n",
    "chat_completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", messages = [{\"role\":\"user\",\"content\":\"Listen to your\"}],max_tokens=1)\n",
    "#This returns a maximum of 1 token\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart.\n",
      "\n",
      "heart,\n",
      "inner voice\n",
      "heart and\n",
      "heart and\n"
     ]
    }
   ],
   "source": [
    "#Setting max tokens\n",
    "#Create chat completion\n",
    "chat_completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", messages = [{\"role\":\"user\",\"content\":\"Listen to your\"}],max_tokens=2,n=5)\n",
    "#n controls the number of responses sent back from the language model\n",
    "\n",
    "#Print the chat completion\n",
    "for i in range(len(chat_completion.choices)):\n",
    "    print(chat_completion.choices[i].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart.\n",
      "heart.\n",
      "heart.\n",
      "heart.\n",
      "heart.\n"
     ]
    }
   ],
   "source": [
    "#Setting max tokens\n",
    "#Create chat completion\n",
    "chat_completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", messages = [{\"role\":\"user\",\"content\":\"Listen to your\"}],max_tokens=2,n=5,temperature=0)\n",
    "#Temperature controls the variance of response\n",
    "\n",
    "#Print the chat completion\n",
    "for i in range(len(chat_completion.choices)):\n",
    "    print(chat_completion.choices[i].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart,\n",
      "heart\n",
      "that commitment\n",
      "one chocolates\n",
      "Answers \n"
     ]
    }
   ],
   "source": [
    "#Setting max tokens\n",
    "#Create chat completion\n",
    "chat_completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", messages = [{\"role\":\"user\",\"content\":\"Listen to your\"}],max_tokens=2,n=5,temperature=2)\n",
    "#Temperature controls the variance of response\n",
    "\n",
    "#Print the chat completion\n",
    "for i in range(len(chat_completion.choices)):\n",
    "    print(chat_completion.choices[i].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot Demo :  Lyric Completion Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial prompt with system message and 2 examples\n",
    "messages_list = [{\"role\":\"system\",\"content\":\"I am Gillella's lyric completion assistant. When given a line from a song, I will provide the next line in the song\"},\n",
    "    {\"role\":\"user\",\"content\":\"First things first rest in peace Uncle Phil\"},\n",
    "    {\"role\":\"assistant\",\"content\":\"For real, you the only father that I ever knew\"},\n",
    "    {\"role\":\"user\",\"content\":\"I get my bitch pregnant I'ma be a better you\"},\n",
    "    {\"role\":\"assistant\",\"content\":\"Prophecies that I made way back in the Ville\"},\n",
    "    {\"role\":\"user\",\"content\":\"Fulfilled, listen even back when we was broke my team ill\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Luther King would've been on Dreamville\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    #create chat completion\n",
    "    chat_completion=openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = messages_list,\n",
    "        max_tokens = 15,\n",
    "        n=1,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "#Print the chat completion\n",
    "print(chat_completion.choices[0].message.content)\n",
    "new_message = {\"role\":\"assistant\",\"content\":chat_completion.choices[0].message.content}\n",
    "#Append new message\n",
    "messages_list.append(new_message)\n",
    "time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Lyrics\n",
    "# First things first rest in peace Uncle Phil\n",
    "# For real, you the only father that I ever knew\n",
    "# I get my bitch pregnant I'ma be a better you\n",
    "# Prophecies that I made way back in the Ville\n",
    "# Fulfilled, listen even back when we was broke my team ill\n",
    "# Martin Luther King would have been on Dreamville\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
