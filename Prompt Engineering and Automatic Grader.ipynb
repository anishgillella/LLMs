{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering is any use of LLM out of the box. it is the art of formatting a prompt to maximise the model's performance on a desired task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models want to complete documents and you can trick them into performing tasks by arranging fake documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 levels fo prompt engineering:\n",
    "1. Easy Way : ChatGPT or other AI models\n",
    "2. Programmatic Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The programmatic way unlocks the new paradign of software development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use LLM to handle logic using prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 tricks to prompt engineering\n",
    "1. Be descriptive\n",
    "- Write me a birthday message for my dad\n",
    "- Write me a birthday message for my dad no longer than 200 characters. This is a big birthday because he is turning 50. To celebrate, I booked a boys' trip to Cancun. Be sure to include some cheeky humor, he loves that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Give examples\n",
    "- Given the title of a Towards Data Science blog article, Write a subtitle for it.\n",
    "Title : Prompt Engineering - How to trick AI into solving your problem\n",
    "Subtitle\n",
    "\n",
    "- Title: A Practical Introduction to LLMs\n",
    "Subtitle : 3 levels of using LLMs in practice\n",
    "\n",
    "- Title: Cracking open the OpenAI(Python) API\n",
    "- Subtitle:A complete beginner-friendly introduction with example code\n",
    "\n",
    "- Title: Prompt Engineering - How to trick AI into solving your problem\n",
    "- Subtitle : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Use structured text\n",
    "4. Chain of thought\n",
    "5. Chatbot Personas\n",
    "6. Flipped Approach\n",
    "7. Reflect, Review and Refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Grader using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAPI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#Define LLM Object\n",
    "chat_model = ChatOpenAI(openai_api_key = \"sk-******************************\",temperature = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Prompt template\n",
    "prompt_template_text = \"\"\"\n",
    "You are a high school history teacher grading homework assignments. Based on the homework question indicated by \\\n",
    "\"**Q:**\" and the correct answer indicated by \"**A:**\", your task is to determine whether the student's answer is correct\\\n",
    "Grading is binary ; therefore, student answers can be correct or wrong. Simple mispellings are okay.\\\n",
    "\n",
    "**Q:** {question}\n",
    "**A:** {correct_answer}\n",
    "\n",
    "**Student's answer:** {student_answer}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= [\"question\",\"correct_answer\",\"student_answer\"],\n",
    "    template= prompt_template_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define chain\n",
    "chain = LLMChain(llm = chat_model, prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Student's answer is wrong.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define inputs\n",
    "question = \"Who was the 35th president of the United States of America\"\n",
    "correct_answer = \"John F Kennedy\"\n",
    "student_answer = \"FDR\"\n",
    "\n",
    "chain.run({\n",
    "    'question': question,\n",
    "    'correct_answer': correct_answer,\n",
    "    'student_answer' : student_answer \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Parser\n",
    "class GradeOutputParser(BaseOutputParser):\n",
    "    \"\"\"Determine whether the answer is correct or wrong\"\"\"\n",
    "\n",
    "    def parse(self,text: str):\n",
    "        \"\"\"Parse the ouptu of an LLM call\"\"\"\n",
    "        return \"wrong\" not in text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update chain\n",
    "chain = LLMChain(\n",
    "    llm = chat_model,\n",
    "    prompt=prompt,\n",
    "    output_parser=GradeOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John F. Kennedy-True\n",
      "\n",
      "\n",
      "JFK-True\n",
      "\n",
      "\n",
      "FDR-False\n",
      "\n",
      "\n",
      "John F. Kennedy-True\n",
      "\n",
      "\n",
      "John Kennedy-True\n",
      "\n",
      "\n",
      "Jack Kennedy-True\n",
      "\n",
      "\n",
      "Jacquelin Kennedy-False\n",
      "\n",
      "\n",
      "Robert F. Kennedy-True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run chain in loop\n",
    "student_answer_list = [\"John F. Kennedy\",\"JFK\",\"FDR\",\"John F. Kennedy\",\"John Kennedy\",\"Jack Kennedy\",\"Jacquelin Kennedy\"\n",
    "                       ,\"Robert F. Kennedy\"]\n",
    "\n",
    "for student_answer in student_answer_list:\n",
    "    print(student_answer + \"-\" + \n",
    "          str(chain.run({\n",
    "              'question': question,\n",
    "              'correct_answer':correct_answer,\n",
    "            'student_answer':student_answer})))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimal prompt strategies are model dependent\n",
    "- Not all pertinent information can fit in context window\n",
    "- General purpose model can be cost inefficient and can overkill\n",
    "- A (smaller) specialized model can out perform a larger general purpose model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
